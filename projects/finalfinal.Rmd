---
title: "STAT 331 Final Project"
author: "Henry Xu, 20779975"
date: "09/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r}
get.reduced.model = function(model, i){
  # convenient helper to return the new model with ith feature removed
  # i can be vector or number
  
  # first column of data will be response variable, other columns are features of original
  # model, intercept wouldn't appear here as a feature
  data = model$model
  r = nrow(data)
  c = ncol(data)
  
  # special case if there is only 1 feature left
  if(c==2){
    return(lm(data[1:r,1]~1))
  }
  
  # we shouldn't receive a model with only intercept
  if(c==1){
    stop("get.reduced.model() recieved a model with intercept only")
  }
  
  # explanatory variable 
  names = colnames(data)[2:c]
  # response variable
  yname = colnames(data)[1]
  formu = as.formula( paste(yname, "~", paste( names[-i], collapse = "+")))
  # new model
  m =  lm(formu , data=data)
  return(m)
}

## note: right now this function could only do 10 fold

get_col <- function(mat,i,j, breaks, cols=NULL, palette="Blues") {
	if (is.null(cols)) {
		cols <- brewer.pal(length(breaks)+1, palette)}
	val <- 1
	for (b in breaks) {
	  if (is.na(mat [i,j])){
	    val <- 0
	  }
	  else if (mat[i,j] > b) {
			val <- val + 1}
			}
	cols[val]
	}

require(RColorBrewer)
col_areas <- function(matrix,
												 breaks=NULL,
												 cols=NULL, 
												 palette="Blues", 
												 xlab="West    <----------->    East",
												 ylab="South   <----------->   North",
												 ...){
	if (is.null(breaks)) {
			breaks <- unique(fivenum(matrix))}
		 	
  plot(c(0, 100*ncol(matrix)),
  			c(0, 100*nrow(matrix)), frame.plot=TRUE,
  			type="n",
  			xlab=xlab, 
  			ylab=ylab, axes=FALSE, ...)
  			
  nr <- nrow(matrix)
  nc <- ncol(matrix)			
	for (i in 1:nr) {
		for (j in 1:nc) {
		    rect((j-1)*100,
		         (nr-i+1)*100,
		         j*100,
		         (nr-i)*100,
		         border=NA,
		         col=get_col(matrix,i,j,breaks,cols,palette))
		         }
		       }
}

```




```{r}
# understanding our polulation:
library("eikosograms")
library("venneuler")
data = read.csv("pollutants.csv")


# change factor features to reasonable names

ind = data$male == 1
data$male[ind] = "M"
data$male[!ind] = "F"
data$agecat = ceiling(data$ageyrs/25 )
agecat = c("<25","25-50","51-75",">75")

for (i in 1:4){
  ind = data$agecat == i
  data$agecat[ind] = agecat[i]
}


edu=c("below", "highsch", "college","grad")
for (i in 1:4){
  ind = data$edu_cat == i
  data$edu_cat[ind] = edu[i]
}

race=c("Other", "Mex", "Black","White")
for (i in 1:4){
  ind = data$race_cat == i
  data$race_cat[ind] = race[i]
}




eikos(edu_cat~ race_cat + male ,data=data)
eikos(edu_cat~ male+agecat ,data=data)



# look at intersection

# note surface of above 45 should be approximately half of surface of total population

collegeabove = which( (data$edu_cat == "college") + (data$edu_cat == "grad") ==1 )
collegeabove.names = rep("collegeabove", length(collegeabove ))

white= which( data$race_cat == "White" )
white.names = rep("White", length(white))

median(data$ageyrs)

above45 = which(data$ageyrs>45)
above45.names= rep("above45", length(above45))

male = which(data$male == "M")
male.names = rep("male", length(male))

female = which(data$male == "F")
female.names = rep("female", length(female))

subjectinfo = c(above45, collegeabove, male)
names = c(above45.names , collegeabove.names, male.names)
ven = venneuler(data.frame(elements = subjectinfo, sets=names))
plot(ven)

# get rid of the agecat data we added
if (colnames(data)[ ncol(data)] == "agecat"){
  data = data[,-ncol(data)]
}



```



```{r}
library(glmnet)
library(car)


data = read.csv("pollutants.csv")


# the index does not really mean anything
data = data[,-1]

nTotal = nrow(data)

#change some feature to factor type
data$race_cat = factor(data$race_cat)
data$edu_cat = factor(data$edu_cat)
data$male = factor(data$male)
data$smokenow= factor(data$smokenow)

summary.stats <- matrix(NA,nrow = ncol(data),ncol = 7)
cov.names <- colnames(data)
for(i in 1:ncol(data)){
  summary.stats[i,1] <- cov.names[i]
  summary.stats[i,2:(1+length(summary(data[,i])))] <- round(summary(data[,i]),2)
}

knitr::kable(summary.stats,caption = "Summary Statistics",
             col.names = c("Name", "Min.", "1st Qu.", 
                           "Median", "Mean", "3rd Qu.","Max."))

for(i in 1:length(cov.names[-1])){
  temp.model <- lm(paste0("length ~ ",cov.names[i+1]),data = data)
  plot(data[,cov.names[i+1]],data$length, main = paste0("Length vs. ",cov.names[i+1]),
       ylab = "Length", xlab = cov.names[i+1])
  abline(temp.model,col = "blue")
}

data.train = data[1:700,]
data.test = data[701:nTotal,]
runif(1)
```



```{r}
# correlation between features
# high correlation -> coeffients have large variance

model = lm(length~. , data=data)
#original vif

vif(model)

t1=colnames( model$model)


while (TRUE) {
  score = vif(model)
  if (max(score) <10){
    break
  }
  ind = which.max(score)
  # this is safe with factor data type
  model = get.reduced.model(model, ind)
}
# reduced model vif
vif(model)
t2=colnames( model$model)

setdiff(t1,t2)
```



```{r}
# does one feature alone explain the model?

# we fit length to each corvariate in a linear/log/square model

Xfull = lm(length~., data=data)$model

res = matrix(0, nrow = (ncol(Xfull)), ncol = 3)


for(c in 2:ncol(Xfull)){
  model = lm(data$length~Xfull[,c])
  #res[,1] is simple linear models
  #res[,2] is log linear models
  #res[,3] is square models
  res[c,1] = mean(model$residuals^2)
  # we won't fit log or sqaure model for catogrical variable because it's bad
  if(! is.factor(Xfull[,c])){
    modelpower2 = lm(data$length~poly( Xfull[,c], 2))
    modellog = lm(log(data$length)~ Xfull[,c])
    res[c,2] = mean(modelpower2$residuals^2)
    res[c,3] = mean(modellog$residuals^2)
  }
}

removezero = function(v){
  v[v==0] = NA
  v
}

# how do these models perform in terms of mse
box = list(simple.linear=removezero(res[,1]), log=removezero(res[,2]), sqr=removezero(res[,3]) )
boxplot(box)

which.min(removezero(res[,1]))
which.min(removezero(res[,2]))
which.min(removezero(res[,3]))


# which is the best single feature
colnames(Xfull)[30]

# what does the best model look like
simplelinear = lm(length~ageyrs, data=data)
plot(data$ageyrs, data$length)
abline(simplelinear$coefficients)

#seems there is a linear relationship but looks insufficient.

#Also seems sqr or log does not do exponentially better here






#how is model choosen by automated soluation



``` 

```{r}


### LASSO
## fit models
M = model.matrix(lm(length~., data=data))
y_train = data$length[1:700]
X_train = M[1:700,-1]
y_test= data$length[701:nTotal]
X_test= M[701:nTotal,-1]

M_lasso <- glmnet(x=X_train,y=y_train,alpha = 1)
####

####
## plot paths
plot(M_lasso,xvar = "lambda",label=TRUE)

## fit with crossval
cvfit_lasso <-  cv.glmnet(x=X_train,y=y_train,alpha = 1)

## plot MSPEs by lambda
plot(cvfit_lasso)

## estimated betas for minimum lambda 
coef(cvfit_lasso, s = "lambda.min")

## predictions
pred_lasso <- predict(cvfit_lasso,newx=X_test,  s="lambda.min")

## MSPE in test set
MSPE_lasso <- mean((pred_lasso-y_test)^2)





## RIDGE
## fit models
M_ridge <- glmnet(x=X_train,y=y_train,alpha = 0)

## plot paths
plot(M_ridge,xvar = "lambda",label=TRUE)

## fit with crossval
cvfit_ridge <-  cv.glmnet(x=X_train,y=y_train,alpha = 0)

## plot MSPEs by lambda
plot(cvfit_ridge)

## estimated betas for minimum lambda 
coef(cvfit_ridge, s = "lambda.min")## alternatively could use "lambda.1se"

## predictions
pred_ridge <- predict(cvfit_ridge,newx=X_test,  s="lambda.min")

## MSPE in test set
MSPE_ridge <- mean((pred_ridge-y_test)^2)


## stepwise

M0 = lm(length~1, data=data.train)
Mfull = lm(length~., data=data.train)
Mstep <- step(object = M0,
              scope = list(lower = M0, upper = Mfull),
              direction = "both", trace = 1, k = 2)

MSPE_step = mean((  predict(Mstep, newdata=data.test) - y_test)^2)

p = predict(Mstep, newdata=data.test)

cvfit_lasso$del

MSPE_lasso
MSPE_ridge
MSPE_step


```







```{r}
# models by automated selection makes little sense for interpretation

#pollutants and bioinfo makes little sense and there are too many covariate



#lets see if there is a smaller good model

#say we try to fit with only 2 features


# lasso choose the same single variable
min(which((M_lasso$lambda)<=exp( -2.5)))
coefs = M_lasso$beta[,4]
which(coefs!=0)


library("plot3D")

# 2 feature lasso choose
i = min(which((M_lasso$lambda)<=exp( -3.96)))
coefs = M_lasso$beta[,i]
choosen=which(coefs!=0)
coefs[choosen]

library(tidyverse)
library(caret)
library(leaps)

models= regsubsets(length~., data=data, nvmax=2)
summary(models)
# rss of all 2 feature model, we see no magical model
mse = models$rss/nrow(data)
hist(mse, main = "Histogram for MSE")
str(models)




# what does the best 2 feature model look like?
z=data$length
y=data$ageyrs
x=data$POP_furan3

fit <- lm(z ~ x + y)
# predict values on regular xy grid
grid.lines = 26
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface

fitpoints = predict(fit)
# scatter plot with regression plane
scatter3D(x, y, z, pch = 18, cex = 2, 
    theta = 20, phi = 20, ticktype = "detailed",
    surf = list(x = x.pred, y = y.pred, z = z.pred,  
    facets = NA, fit = fitpoints), xlab="pop_furan3", ylab="ageyrs",zlab="length")


#turn ageyrs and pop_furan into grids

miny=min(y)
intervaly = (max(y)-miny)/10
minx=min(x)
intervalx = (max(x)-minx)/10

xy = matrix(0, nrow = 10, ncol = 10)
count = matrix(0, nrow = 10, ncol = 10)
for (i in 1:nrow(data)){
  xgrid = (x[i]-minx)/intervalx
  ygrid = (y[i]-miny)/intervaly
  count[xgrid,ygrid] = 1 + count[xgrid,ygrid]
  xy[xgrid,ygrid] = xy[xgrid, ygrid] + z[i]
}
xygrid = xy/count
col_areas(xygrid,xlab="pop_furan3", ylab="ageyrs")
maxz=max(z)
minz=min(z)
breaks = seq( minz, maxz, by=(maxz-minz)/5 )
col_areas(xygrid,xlab="pop_furan3", ylab="ageyrs", breaks = breaks)


# anyway how does this compare to the best fit?


```




```{r}
# 4/4

# perhaps pollutants is related to length
# we try it

# pollutants values are very large, we log transform it. and erroranalysis looks better

cols = colnames(data) 
po.ind = str_detect(cols, "POP")
seed <- "20779975"
# this is to test transformation of data's result on lasso result
# limitation: transformation of other feature, some we cannot transform because they have value 0 or negative
lasso.on.pollutants =function(data_1){
  set.seed(seed)
  M = model.matrix(lm(length~., data=data_1))
  cols = colnames(M) 
  po.ind = str_detect(cols, "POP")
  y_train = data_1$length[1:700]
  X_train = M[1:700,po.ind]
  y_test= data_1$length[701:nTotal]
  X_test= M[701:nTotal,(1:ncol(M))[po.ind]]
  
  
  M_lasso <- glmnet(x=X_train,y=y_train,alpha = 1)
  ## plot paths
  
  ## fit with crossval
  cvfit_lasso <-  cv.glmnet(x=X_train,y=y_train,alpha = 1)
  
  ## plot MSPEs by lambda
  
  ## estimated betas for minimum lambda 
  
  ## predictions
  pred_lasso <- predict(cvfit_lasso,newx=X_test,  s="lambda.min")
  
  ## MSPE in test set
  MSPE_lasso <- mean((pred_lasso-y_test)^2)
  print(paste("mspe",MSPE_lasso) )
  
  plot(pred_lasso, y_test)
  
  return( coef(cvfit_lasso, s = "lambda.min"))
  
}



#########################


# log transform
newdata = data
newdata$length = log(newdata$length)
newdata[,po.ind] = log(data[,po.ind])
newdata2 = data
newdata2[,po.ind] = log(data[,po.ind])
chosen.po.ind= which(lasso.on.pollutants(newdata)!=0)
chosen.po.ind= chosen.po.ind[2:length(chosen.po.ind)]


``` 



```{r}
## Influence
## determining outliers
plot.outliers <- function(M){
  Xmat <- model.matrix(M) ## design matrix
  H <- Xmat%*%solve(t(Xmat)%*%Xmat)%*%t(Xmat) ## Hat matrix
  diag(H)
  lev <- hatvalues(M) ## leverage (h_i)
  hbar <- mean(lev) ## \bar{h}
  c(sum(lev),ncol(model.matrix(M)))## check trace is same as rank of 
  
  ## plot leverage
  plot(lev,ylab="Leverage")
  abline(h=2*hbar,lty=2) ## add line at 2hbar
  ids <- which(lev>2*hbar) ## x values for labelling points >2hbar
  points(lev[ids]~ids,col="red",pch=19) ## add red points >2hbar
  text(x=ids,y=lev[ids], labels=ids, cex= 0.6, pos=2) ## label points >2hbar  
}
outliers <- function(M){
  Xmat <- model.matrix(M) ## design matrix
  H <- Xmat%*%solve(t(Xmat)%*%Xmat)%*%t(Xmat) ## Hat matrix
  diag(H)
  lev <- hatvalues(M) ## leverage (h_i)
  hbar <- mean(lev) ## \bar{h}
  c(sum(lev),ncol(model.matrix(M)))## check trace is same as rank of 
  which(lev > 2*hbar)
}

plot.jackknife.res <- function(M){
  res <- resid(M) # raw residuals
  
  Xmat <- model.matrix(M) ## design matrix
  H <- Xmat%*%solve(t(Xmat)%*%Xmat)%*%t(Xmat) ## Hat matrix
  diag(H)
  lev <- hatvalues(M) ## leverage (h_i)
  hbar <- mean(lev) ## \bar{h}
  ids <- which(lev>2*hbar) ## x values for labelling points >2hbar
  n <- nobs(M)
  p <- length(attr(terms(M),"term.labels"))
  stud <- res/(sigma(M)*sqrt(1-lev)) # studentized residuals
  jack <- stud*sqrt((n-p-2)/(n-p-1-stud^2)) 
  plot(jack,ylab="Studentized Jackknife Residuals")
  points(jack[ids]~ids,col="red",pch=19) ## add high leverage points
  text(ids,jack[ids], labels=ids, cex= 0.6, pos=2) ## label points >2hbar
}

jackknife.res <- function(M){
  res <- resid(M) # raw residuals
  
  Xmat <- model.matrix(M) ## design matrix
  H <- Xmat%*%solve(t(Xmat)%*%Xmat)%*%t(Xmat) ## Hat matrix
  diag(H)
  lev <- hatvalues(M) ## leverage (h_i)
  hbar <- mean(lev) ## \bar{h}
  ids <- which(lev>2*hbar)
  return(ids)
}
plot.outliers(Mstep)
plot.jackknife.res(Mstep)

```


```{r}
## helpful functions for plotting influence
##----------------DFFITS-----------------------------------------------------------
# Calculates influential points based on DFFITS.
DFFITS <- function(M,method = 1, cutoff = 0.05){
  data <- M$model
  p <- length(attr(terms(M),"term.labels"))
  n <- nobs(M)
  ## check leverage
  h <- hatvalues(M)
  ##----------------DFFITS-----------------
  dffits_m <- dffits(M)
  if(method == 1){
  cutoff <- 2*sqrt((p+1)/n)
  }
  which(abs(dffits_m)>cutoff)
}

plot.DFFITS <- function(M, method = 1, cutoff = 0.05){
  data <- M$model
  p <- length(attr(terms(M),"term.labels"))
  n <- nobs(M)
  ## check leverage
  h <- hatvalues(M)
  
  
  dffits_m <- dffits(M) 
  
  if(method == 1){
    cutoff <- 2*sqrt((p+1)/n)
  }
  
  ## plot DFFITS
  plot(dffits_m,ylab="DFFITS") 
  abline(h=cutoff,lty=2, col = "red")  ## add thresholds
  abline(h=-cutoff,lty=2, col = "red")
  ## highlight influential points
  dff_ind <- which(abs(dffits_m)>cutoff)
  points(dffits_m[dff_ind]~dff_ind,col="red",pch=19) ## add red points
  text(y=dffits_m[dff_ind],x=dff_ind, labels=dff_ind, pos=2) ## label high influence points
  abline(h = cutoff, col = "red", lty = 2)
  abline(h = -cutoff, col = "red", lty = 2)
}


##----------------Cook's Distance--------------------------------------------------------
# Calculates influential points based on Cook's Distance
CD <- function(M, cutoff = 0.5){
  p <- length(attr(terms(M),"term.labels"))
  n <- nobs(M)
  D <- cooks.distance(M) # Cook's distance
  ## influential points
  which(pf(D,p+1,n-p-1,lower.tail=TRUE)>cutoff)
}

plot.CD <- function(M,method = 1, cutoff = 0.5){
  # method = 1 is default (may not print any influential points if cutoff is not low enough)
  # method = else <- calculate using simple R method
  if(method == 1){
    p <- length(attr(terms(M),"term.labels"))
    n <- nobs(M)
    D <- cooks.distance(M) # Cook's distance
    ## influential points
    inf_ind <- which(pf(D,p+1,n-p-1,lower.tail=TRUE)>cutoff)
    
    ## plot cook's Distance
    plot(D,ylab="Cook's Distance")
    points(D[inf_ind]~inf_ind,col="red",pch=19) ## add red points
    text(y=D[inf_ind],x=inf_ind, labels=inf_ind, pos=4) ## label high influence points    
  }else{
    plot(M,which = 4)
  }
}


##----------------DFBETAS-----------------------------------------------------------
# Calculates influential points based on DFBETAS.
DFBETAS <- function(M, method = 1, cutoff = 0.05){
  DFBETAS <- dfbetas(M) 
  dim(DFBETAS)
  n <- nobs(M)
  
  # method = 1 <- default cutoff 2/sqrt(n)

  if(method == 1){
    cutoff <- 2/sqrt(n)
  }
  
  vals <- list()
  for(i in 2:dim(DFBETAS)[2]){
    vals[[i]] <- which(abs(DFBETAS[,i])>cutoff)
  }
  vals
}

plot.DFBETAS <- function(M, method = 1, cutoff = 0.05){
  
  n <- nobs(M)
  
  # method = 1 <- default cutoff 2/sqrt(n)
  if(method == 1){
    cutoff <- 2/sqrt(n)
  }
  
 
  DFBETAS <- dfbetas(M) 
  dim(DFBETAS)
  ## beta1
  for(i in 2:dim(DFBETAS)[2]){
    plot(DFBETAS[,i], type="h",xlab="Obs. Number", 
         ylab=bquote(beta[.(i)]), main = "DFBETAS")
    show_points <- which(abs(DFBETAS[,i])>cutoff)
    points(x=show_points,y=DFBETAS[show_points,i],pch=19,col="red")
    abline(h = cutoff, col = "red", lty = 2)
    abline(h = -cutoff, col = "red", lty = 2)
    text(x=show_points,y=DFBETAS[show_points,i],labels=show_points,pos=2)
  }
}


```



```{r}

# Error Analysis
errorAnalysis <- function(M){
  ## residuals
  newdata <- M$model
  res1 <- resid(M) # raw residuals
  stud1 <- res1/(sigma(M)*sqrt(1-hatvalues(M))) # studentized residuals
  
  ## plot distribution of studentized residuals
  hist(stud1,breaks="FD",
     probability=TRUE,xlim=c(-4,4),
     xlab="Studentized Residuals",
     main="Distribution of Residuals")
  grid <- seq(-3.5,3.5,by=0.05)
  lines(x=grid,y=dnorm(grid),col="blue") # add N(0,1) pdf
  
  ## qqplot of studentized residuals
  qqnorm(stud1)
  abline(0,1) # add 45 degree line
  
  ## plot of residuals vs X
  factors <- attr(terms(M),"term.labels")
  for(i in 1:length(factors)){
    ind <- which(colnames(newdata)==factors[i]) 
    plot(res1 ~ newdata[,ind],ylab = "residuals",
         xlab = factors[i], main = paste0("Residuals vs ",factors[i]), ylim = c(-1,2))
  }
  
  ## plot of studentized residuals vs fitted values
  plot(stud1~fitted(M),
     xlab="Fitted Vals",
     ylab="Studentized Residuals",
     main="Residuals vs Fitted")

}
```


```{r}
kfolds.cv <- function(dat, expr){
  kfolds=10
  mspe = rep(0, kfolds)
  ind = rep(1:kfolds, length=nrow(dat))
  for(ii in 1:kfolds) {
    train<- which(ind!=ii) # training observations
    M.cv <- lm(expr, data=data[train,])
    # cross-validation residuals
    M.res <- dat$length[-train] - # test observations
      predict(M.cv, newdat = dat[-train,]) # prediction with training dat
    # mspe
    mspe[ii] <- mean(M.res^2)
  }
  mean(mspe)
}


# limits:
# only contains history of beta values of initial features
# forward selection won't remove already-added features

forward.change = function(data, expr, show=FALSE){
  model = lm(expr, data=newdata)
  initial.colname = names( model$coefficients)[-1]
  tempnames = colnames(data)
  cv.hist=c()
  aic.hist = c()
  coef.hist = list()
  DFFITS.hist = c()
  outliers.hist = c()
  j=0
  models = list()
  while (TRUE) {
    j=j+1
    # FOR METHODS EXPLANATIONprint(paste("step", j))
    cov.in.m = colnames(model$model)
    cov.all = colnames(newdata)
    names.to.try = cov.all[! cov.all %in% cov.in.m]
    nn = length(names.to.try)
    #update tracks
    cv.hist[j]=kfolds.cv(newdata, expr)
    aic.hist[j] = extractAIC(model)[2]
    coef.hist[[j]] = coef(model)
    DFFITS.hist[j] = length(DFFITS(model))
    outliers.hist[j] = length(outliers(model))
    models[[j]] = model
    cv.score = rep(0, nn)
    if(length(names.to.try) == 0){
      # FOR METHODS EXPLANATIONprint("chose all ")
      break
    }
    for (i in 1:nn) {
      name = names.to.try[i]
      newexpr =   paste(expr,  "+", name ) 
      newmodel = lm(newexpr, data=newdata)
      cv.score[i] = kfolds.cv(newdata, newexpr)
    }
    ind = which.min(cv.score)
    if(cv.score[ind]>cv.hist[j]){
      # FOR METHODS EXPLANATIONprint ("done choosing model")
      break
    }else{
      # update our model
      print(paste("added", names.to.try[ind]))
      expr = paste(expr,"+", names.to.try[ind])  
      model =  lm(expr, data=newdata)
    }
  }
  plot(cv.hist, main = "cv")
  plot(aic.hist, main = "aic")
  plot(DFFITS.hist, main = "# of Influential Points - DFFITS")
  plot(outliers.hist, main = "# of Outliers")
  i = length(initial.colname)
  j = length(coef.hist)
  M = matrix(0, nrow = i, ncol = j)
  for (ii in 1:i){
    for (jj in 1:j) {
      M[ii,jj] =  coef.hist[[jj]][initial.colname[ii]]
    }
  }
  if(show==TRUE){
    par(cex=0.7)
    plot(M[1,], main="coefficent of pollutants", type = 'l', col=1, ylim = range(M))
    if(i!=1){
      for (a in 2:i){
        lines(1:j, M[a,] ,col=a)
      }
      legend("topright",legend = initial.colname, col = 1:i, pch=1)
    }
  }
  return(list(cv=cv.hist, coef=coef.hist, aic=aic.hist,
              outliers=outliers.hist, DFFITS = DFFITS.hist,
              models = models))
}


```



```{r}
# we will analysis these
set.seed(seed)

# log transform
newdata=data
newdata$length <- log(newdata$length)
newdata[,po.ind] = log(newdata[,po.ind])

# start from chosen pollutens 
chosen.pos = colnames(newdata)[chosen.po.ind]
expr = paste("length~", paste(chosen.pos, collapse = "+"))
lasso.pollu.model = (lm(expr,data=newdata))


t=forward.change(newdata, expr, TRUE)
# the last step vary by a lot becasue large aif -> large variance on beta, we shouldn't consider last step as good


# forward start from lm(length~1) done
# chosen pollute + other by forward done
# error analysis 
# visualize the smoke stuff
# how the coefficients vary 

# path of the "error analysis stuff/ cook's distance dffits"

finalModel <- t$models[[2]]

summary(finalModel)

summary(lm(data$length~data$POP_furan4))

finalModel$coefficients[ finalModel$coefficients<0.01]
finalModel$coefficients[ finalModel$coefficients>0.01]

#vif(finalModel)


numpara= length(lasso.pollu.model$coefficients)-1
# excluding length in full model
plot(rep(1:numpara,2),  c(lasso.pollu.model$coefficients[-1],finalModel$coefficients[-c(1,numpara+2)] ) , 
     col=rep(c(1,2), each=numpara), xlab = "feature index", ylab="coefficient", main = "coefficient after forward selection")
legend("bottomright", legend=c("before", "after"), col=c(1,2), lty=3)

coef(lasso.pollu.model)
coef(finalModel)


newdata=data
newdata[,po.ind] = log(newdata[,po.ind])
chosen.pos = colnames(newdata)[chosen.po.ind]
expr = paste("length~", paste(chosen.pos, collapse = "+"))

t=forward.change(newdata, expr, TRUE)

```


```{r, fig.height = 5.5, fig.width = 8}
errorAnalysis(t$models[[2]])

plot(x = finalModel$fitted.values, y = log( data$length), 
     main = "Fitted values vs. Log Length")
abline(a = 0, b = 1,col = "red", lwd= 2)
newdata2 <- newdata
newdata2$length <- log(newdata2$length)


```

```{r}
# our final model
newdata=data
newdata[,po.ind]=log(data[,po.ind])
newdata[,1]=log(data[,1])
chosen.po.ind=lasso.on.pollutants(newdata)
chosen.po.ind
expr = paste("length~", paste(chosen.pos, collapse = "+"))
t =forward.change(newdata, expr,TRUE)


newdata=data
newdata[,po.ind]=log(data[,po.ind])
chosen.po.ind=lasso.on.pollutants(newdata)
chosen.po.ind
expr = paste("length~", paste(chosen.pos, collapse = "+"))
t =forward.change(newdata, expr,TRUE)




```

